{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.1 Знакомство с линейным классификатором\n",
    "### 1. Как выглядит бинарный линейный классификатор? (Формула для отображения из мно-жества объектов в множество классов.)\n",
    "-- Общий вид $a(x) = sign(f(x))$, где $f(x)$- дискриминантная функция. При $a(x) = sign(<w, x> + w_0)$, $f(x) = <w,x> + w_0$ - линейная.\n",
    "\n",
    "### 2. Что такое отступ алгоритма на объекте? Какие выводы можно сделать из знака отступа?\n",
    "-- Отступ на объекте -- величина $M_i = y_i \\cdot f(x_i)$. При $a(x_i) \\neq y_i \\Leftrightarrow M_i \\leq 0$\n",
    "\n",
    "### 3. Как классификаторы вида $a(x) = sign(<w, x> − w_0)$ сводят к классификаторам вида $a(x) = sign(<w, x>)$?\n",
    "-- Добавляют $x_0 = 1$  к вектору $x$, как еще одну координату, к вектору весов соответственно $w_0$.\n",
    "\n",
    "### 4. Как выглядит запись функционала эмпирического риска через отступы? Какое значение он должен принимать для «наилучшего» алгоритма классификации?\n",
    "--$Q(a, X^l) = \\frac{1}{l}\\cdot \\sum_{i=1}^{n}[a(x_i) \\neq y_i] = \\frac{1}{l} \\cdot \\sum_{i=1}^{n}[M_i \\leq 0]$ , равен $0$ для наилучшего алгоритма классификации (все точки классифицированы правильно, т.е. каждый отстут отрицательный).\n",
    "\n",
    "### 5. Если в функционале эмпирического риска (риск с пороговой функцией потерь) всюду написаны строгие неравенства ($M_i < 0$) можете ли вы сразу придумать параметр w для алгоритма классификации $a(x) = sign(< w, x >)$, минимизирующий такой функционал?\n",
    "--$w=0$\n",
    "\n",
    "### 6. Запишите функционал аппроксимированного эмпирического риска, если выбрана функция потерь L(M ).\n",
    "--$Q(a, X^l) = \\frac{1}{l}\\cdot \\sum_{i=1}^{l}L(M)$\n",
    "\n",
    "### 7. Что такое функция потерь, зачем она нужна? Как обычно выглядит ее график?\n",
    "--Функция потерь $L(a, x)$ = величине ошибки алгоритма $a$ на объекте $x$. L - неотрицательна, для линейной классификации - ($L(M)$) - невозрастающая. Обычно выпукла, для хорошего решения оптимизации.\n",
    "\n",
    "### 8. Приведите пример негладкой функции потерь.\n",
    "--$L(M) = [M\\leq 0]$ - функция Хевисайда - пороговая функция потерь.\n",
    "\n",
    "### 9. Что такое регуляризация? Какие регуляризаторы вы знаете?\n",
    "-- Регуляризатор - величина штрафа за сложность модели. Позволяет бороться с переобучением, не давая модели быть слишком сложной( например высокая степень полинома в задаче интерполяции). В случае линейных моделей при переобучение наблюдаются большие веса, поэтому регуляризатор - штраф за большие веса, ограничивает дискриминантную функцию.\n",
    "Примеры:\n",
    "    $$l_0-регуляризатор -- \\lambda \\cdot \\sum_{i=1}^{n}[w_i \\neq 0]$$\n",
    "    $$l_1-регуляризатор -- \\lambda \\cdot \\sum_{i=1}^{n} \\mid w_i \\mid$$,\n",
    "    $$l_2-регуляризатор -- \\lambda \\cdot \\sum_{i=1}^{n} w_i^2$$\n",
    "    \n",
    "### 10. Как связаны переобучение и обобщающая способность алгоритма? Как влияет регуляризация на обобщающую способность?\n",
    "--Обобщающая способность алгоритма $a$ = $Q(a(X^l), X^k)$, $X^l, X^k$ - непересекающиеся выборки. При переобучении это значение будет большим (возможно из-за увеличения одного из весов). Регуляризация борется с переобучением, а значит и ограничивает значение этого функционала.\n",
    "\n",
    "### 11. Как связаны острые минимумы функционала аппроксимированного эмпирического риска с проблемой переобучения?\n",
    "--Выход из минимума приводит к сильному возрастанию функционала аппроксимированного эмпирического риска, что приводит к большому весу и переобучению.\n",
    "\n",
    "### 12. Что делает регуляризация с аппроксимированным риском как функцией параметров алгоритма?\n",
    "--Увеличивает при приближении параметров алгоритма к недопустимым границам.\n",
    "\n",
    "### 13. Для какого алгоритма классификации функционал аппроксимированного риска будет принимать большее значение на обучающей выборке: для построенного с регуляризацией или без нее? Почему?\n",
    "--С регуляризацией, так как она увеличивает значение функционала риска.\n",
    "\n",
    "### 14. Для какого алгоритма классификации функционал риска будет принимать большее значение на тестовой выборке: для построенного с оправдывающей себя регуляризацией или вообще без нее? Почему?\n",
    "-- Бывает по-разному. Если без регуляризации алгоритм сильно переобучался, то и на тестовой выборке даст плохие результаты и большое значение функицонала риска. С другой стороны, если переобучение без регуляризации не превосходит вес от регуляризации, то может быть большой значение с регуляризацией.\n",
    "\n",
    "### 15. Что представляют собой метрики качества Accuracy, Precision и Recall?\n",
    "-- Допустим метки классов +1 и -1. \n",
    "\n",
    "    TP - True Positive - предсказан +1, правильно +1,\n",
    "\n",
    "    FN - False Negative - предсказан -1, правильно +1,\n",
    "    \n",
    "    TN - True Negative - предсказан -1, правильно -1,\n",
    "    \n",
    "    FP - False Positive - предсказан +1, правильно -1\n",
    "    \n",
    "    P - Positive = размер класса +1,\n",
    "    \n",
    "    N - Negative = размер класса -1\n",
    "    \n",
    "Тогда\n",
    "$$Accuracy = \\frac{TP+TN}{P+N}$$ - доля правильных ответов\n",
    "$$Precision = \\frac{TP}{TP+FP}$$ - точность - сколько из предсказанно правильно\n",
    "$$Recall = \\frac{TP}{P}$$ - полнота - сколько из правильных предсказали\n",
    "    \n",
    "### 16. Что такое метрика качества AUC и ROC-кривая?\n",
    "--$$False Positive Rate = FPR = \\frac{FP}{N}$$\n",
    "$$True Positive Rate = TPR = \\frac{TP}{P}$$.\n",
    "ROC-кривая - график зависимости TPR(FPR). ROC-AUC - метрика качества - площадь под ROC-кривой.\n",
    "\n",
    "### 17. Как построить ROC-кривую (нужен алгоритм), если например, у вас есть правильные ответы к домашнему заданию про фамилии и ваши прогнозы?\n",
    "--По ответам получим множество точек $\\{(FPR_i, TPR_i)\\}_{i=0}^{l}$, по которой строится ROC-кривая.\n",
    "\n",
    "    1) (FPR_0, TPR_0) = (0, 0)\n",
    "    2) P - число фамилий в выборке, N - не фамилий\n",
    "    3) Отсортием выборку по значениям f(x_i)\n",
    "    4) for i in range(1, l):\n",
    "    5)     if y_i == 1:\n",
    "    6)         (FPR_i, TPR_i) = (FPR_{i-1}, TPR_{i-1} + 1/P)\n",
    "    7)     else:\n",
    "    8)         (FPR_i, TPR_i) = (FPR_{i-1} + 1/N, TPR_{i-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.2 Вероятностный смысл регуляризаторов\n",
    "### Покажите, что регуляризатор в задаче линейной классификации имеет вероятностный смысл априорного распределения параметров моделей. Какие распределения задают l1-регуляризатор и l2-регуляризатор?\n",
    "\n",
    "--Решение\n",
    "\n",
    "Допустим, множество $X\\times Y$ является вероятностным пространством, и заданы плотности распределения объектов и классов $p(x, y|w)$. Введем параметрическое семейство априорных распределений $p(w; \\gamma)$, где $\\gamma$ — параметр.\n",
    "Логарифм функции правдоподобия:\n",
    "$$L_{\\gamma}(X^l, w) = \\ln p(X^l, w; \\gamma) = \\sum_{i=1}^{l} p(x_i, y_i | w) + \\ln p(w; \\gamma) \\rightarrow max$$\n",
    "Ищем максимум этой функции. Последнее слагаемое представляет собой регуляризатор. Так как принцип максимального праводопобия эквивалентен принципу минимизации аппроксимированного эмпирического риска \n",
    "$$ Q(X^l, w) = \\sum_{i=1}^{l} L(y_i \\cdot f(x_i, w)) + \\gamma \\cdot V(w) \\rightarrow min$$\n",
    "\n",
    "Сопоставляя принципы и формулы получаем\n",
    "$$-\\ln p(x_i, y_i|w) = L(y_i \\cdot f(x_i, w))$$\n",
    "$$\\ln p(w;\\gamma) = \\gamma \\cdot V(w)$$\n",
    "\n",
    "Таким образом, получаем, что регуляризатор $V(w)$ соответствует параметрическому семейству\n",
    "априорных распределений плотностей $p(w; \\gamma)$ — параметров моделей.\n",
    "\n",
    "Для $l_1$-регуляризатора\n",
    "Пусть $w \\in R$ имеет $n$-мерное распределение Лапласа:\n",
    "$$p(w; C) = \\frac{1}{(2C)^n} e^{-\\frac{\\|w\\|_1}{C}}, \\|w\\|_1 = \\sum_{i=1}^{n} |w_j|$$\n",
    "\n",
    "т.е. все веса независимы, имеют нулевое матожидание и равные дисперсии; C — гиперпараметр.\n",
    "Логарифмируя, получаем регуляризатор по $l_1$ -норме:\n",
    "$$-\\ln p(w; C) = \\frac{1}{C} \\sum_{i=1}^{n} |w_j| + const(w) $$\n",
    "\n",
    "\n",
    "Для $l_2$-регуляризаторa\n",
    "Пусть $w \\in R$ имеет $n$-мерное гауссовское распределение:\n",
    "$$ p(w; \\sigma) = \\frac{1}{(2 \\pi \\sigma)^\\frac{n}{2}} e^{-\\frac{\\|w\\|^2}{2\\sigma^2}}, \\|w\\|^2 = \\sum_{i=1}^{n} w_i^2$$\n",
    "\n",
    "т.е. все веса независимы, имеют нулевое матожидание и равные дисперсии $\\sigma$; $\\sigma$ — гиперпараметр.\n",
    "Логарифмируя, получаем регуляризатор по $l_2$ -норме:\n",
    "$$−\\ln p(w; \\sigma) = \\frac{1}{2\\sigma^2} \\|w\\|^2 + const(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.3 SVM и максимизация разделяющей полосы\n",
    "### Покажите, как получается условная оптимизационная задача, решаемая в SVM из соображений максимизации разделяющей полосы между классами. Можно отталкиваться от линейно разделимого случая, но итоговое выражение должно быть для общего. Как эта задача сводится к безусловной задаче оптимизации?\n",
    "\n",
    "--Решение\n",
    "\n",
    "\n",
    "Рассмотрим задачу классификации на два непересекающихся класса, в которой объекты опи-\n",
    "сываются $n$-мерными вещественными векторами: $X \\in R^n, Y = \\{−1, +1\\}$.\n",
    "Будем строить линейный пороговый классификатор:\n",
    "$$a(x) = sign(<w, x> - w_0)$$\n",
    "\n",
    "Для начала предположим, что выборка линейна разделима: найдутся $w$, $w_0$ , задающие разде-\n",
    "ляющую гиперплоскость $⟨w, x⟩ = w_0$ , при которых функционал числа ошибок\n",
    "$$Q(w, w_0) = \\sum_{i=1}^{l} [y_i \\cdot (⟨w, x_i⟩ − w_0 ) \\leq 0] = 0 $$\n",
    "Найдем, как оптимальнее расположить разделяющую гиперплоскость. Для простоты выполним\n",
    "нормировку параметров алгоритма: домножим $w$ и $w_0$ на такую константу, что\n",
    "$$min (y_i \\cdot (⟨w, x_i⟩ − w_0)) = 1, i \\in \\overline{1, l}$$\n",
    "Хочется максимизировать ширину разделяющей полосы. Тогда на границе разделяющей поло-\n",
    "сы будут лежать точки из обучающей выборки: $x_{−},x_{+}$ , принадлежащие соответственно классам -1, +1. Ширина полосы\n",
    "$$ \\left< (x_{+} - x_{-}), \\frac{w}{\\|w\\|} \\right> = \\frac{<w, x_{+}> - <w, x_{-}>}{\\|w\\|} = \\frac{(w_0+1)-(w_0-1)}{\\|w\\|} = \\frac{2}{\\|w\\|}$$, тут мы использовали условие минимума.\n",
    "\n",
    "Получаем, что ширина полосы максимальна, когда норма вектора w минимальна. Значит, мож-\n",
    "но сформулировать следующую задачу оптимизации:\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   & \\frac{1}{2}\\cdot \\|w\\|^2 \\rightarrow min \\\\\n",
    "   & y_i \\cdot (<w, x_i> - w_0) \\geq 1, i \\in \\overline{1, l}\n",
    " \\end{cases}\n",
    "\\end{equation*} \n",
    "\n",
    "Если работать с линейно неразделимой выборкой, $y_i \\cdot (⟨w, x_i⟩ − w_0)$ не обязательно будет не меньше 1. Ослабим эти ограничения и введем в минимизируемый функционал штраф за суммарную ошибку:\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   & \\frac{1}{2}\\cdot \\|w\\|^2 + C \\sum_{i=1}^{l} \\xi_i \\rightarrow min \\\\\n",
    "   & M_i = y_i \\cdot (<w, x_i> - w_0) \\geq 1 - \\xi_i, i \\in \\overline{1, l} \\\\\n",
    "   & \\xi_i \\geq 0\n",
    " \\end{cases}\n",
    "\\end{equation*} \n",
    "Где $\\xi_i$ - величина ошибки на $x_i$\n",
    "Из 2 и 3 условий:\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   & \\xi_i \\geq 1 - M_i\\\\\n",
    "   & \\xi_i \\geq 0\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "Тогда $\\xi_i$ минимальна при $\\xi_i = (1 - M_i)_{+}$\n",
    "\n",
    "\n",
    "Исходная задача тогда имеет вид:\n",
    "$$\\frac{1}{2}\\cdot \\|w\\|^2 + C\\cdot \\sum_{i=1}^{n}(1-M_i)_{+} \\rightarrow min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.4 Kernel trick\n",
    "### Придумайте ядро, которое позволит линейному классификатору с помощью Kernel Trick построить в исходном пространстве признаков разделяющую поверхность $x_1^2 + 2 x_2^2 = 3$. Какой будет размерность спрямляющего пространства?\n",
    "\n",
    "--Решение\n",
    "\n",
    "Возьмем квадратичное ядро: $$K(x, y) = ⟨x, y⟩^2 = (x_1 \\cdot y_1 + x_2 \\cdot y_2)^2 = x_1^2 \\cdot y_1^2 + 2 x_1 y_1 x_2 y_2 + x_2^2 \\cdot y_2^2 = ⟨(x_1^2, x_2^2, \\sqrt{2}x_1 x_2), (y_1^2, y_2^2, \\sqrt{2}y_1 y_2)⟩$$\n",
    "Получим отображение в спрямляющее пространство $H = R^3$:\n",
    "$$ \\psi(x_1, x_2) \\rightarrow (x_1^2, x_2^2, \\sqrt{2}x_1 x_2)$$\n",
    "Тогда линейная поверхность в $H$ будет иметь вид: $$⟨(x_1^2, x_2^2, \\sqrt{2} x_1 x_2), (w_1, w_2, w_3)⟩ + w_0 = w_1 x_1^2 + w_2 x_2^2 + w_3 \\sqrt{2} x_1 x_2 + w_0 = x_1^2 + 2 x_2^2 - 3 = 0, w=(1, 2, 0), w_0 = -3$$\n",
    "Размерность 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.5 $l_1$ -регуляризация\n",
    "### Покажите с помощью теоремы Куна-Таккера, что ограничение 1 -нормы вектора весов числом и добавление штрафа с его 1 -нормой приводят к построению одного и того же алгоритма. Можно считать, что регуляризатор добавляется по существу, т.е. меняет итоговый ответ по сравнению с оптимизационной задачей без регуляризатора.\n",
    "\n",
    "--Решение\n",
    "\n",
    "Покажите с помощью теоремы Куна-Таккера, что LASSO Тибширани и l1-регуляризация ли-\n",
    "нейной регрессии приводят к построению одного и того же алгоритма.\n",
    "Решение\n",
    "Лассо Тибширани:\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   & Q(\\alpha) = \\|F\\alpha - y\\|^2 \\rightarrow min(\\alpha) \\\\\n",
    "   & \\sum_{i=1}^{n} |\\alpha_j| \\leq \\kappa \n",
    " \\end{cases}\n",
    "\\end{equation*} \n",
    "По теореме Куна-Таккера:\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   & Q(\\alpha) = \\|F\\alpha - y\\|^2 + \\lambda \\cdot \\left(\\sum_{i=1}^{n} |\\alpha_j| - \\kappa \\right) = \\|F\\alpha - y\\|^2 + \\lambda \\cdot \\sum_{i=1}^{n} |\\alpha_j| + const \\rightarrow min(\\alpha) \\\\\n",
    "   & \\lambda \\geq 0\\\\\n",
    "   & \\lambda \\cdot \\left(\\sum_{i=1}^{n} |\\alpha_i| - \\kappa \\right) = 0 \\Leftrightarrow \\lambda = 0 \\text{или} \\sum_{i=1}^{n} |\\alpha_i| = \\kappa\n",
    " \\end{cases}\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.6 Повторение: метрики качества\n",
    "### 1. Что представляют собой метрики качества Accuracy, Precision и Recall?\n",
    "### 2. Что такое метрика качества AUC и ROC-кривая?\n",
    "### 3. Как построить ROC-кривую (нужен алгоритм), если например, у вас есть правильные ответы к домашнему заданию про фамилии и ваши прогнозы?\n",
    "\n",
    "### 1.\n",
    "Допустим метки классов +1 и -1. \n",
    "\n",
    "    TP - True Positive - предсказан +1, правильно +1,\n",
    "    \n",
    "    FN - False Negative - предсказан -1, правильно +1,\n",
    "    \n",
    "    TN - True Negative - предсказан -1, правильно -1,\n",
    "    \n",
    "    FP - False Positive - предсказан +1, правильно -1\n",
    "    \n",
    "    P - Positive = размер класса +1,\n",
    "    \n",
    "    N - Negative = размер класса -1\n",
    "\n",
    "Тогда\n",
    "$$Accuracy = \\frac{TP+TN}{P+N}$$ - доля правильных ответов\n",
    "$$Precision = \\frac{TP}{TP+FP}$$ - точность - сколько из предсказанно правильно\n",
    "$$Recall = \\frac{TP}{P}$$ - полнота - сколько из правильных предсказали\n",
    "    \n",
    "### 2.\n",
    "$$False Positive Rate = FPR = \\frac{FP}{N}$$\n",
    "$$True Positive Rate = TPR = \\frac{TP}{P}$$.\n",
    "ROC-кривая - график зависимости TPR(FPR). ROC-AUC - метрика качества - площадь под ROC-кривой.\n",
    "\n",
    "### 3.\n",
    "По ответам получим множество точек $\\{(FPR_i, TPR_i)\\}_{i=0}^{l}$, по которой строится ROC-кривая.\n",
    "\n",
    "    1) (FPR_0, TPR_0) = (0, 0)\n",
    "    2) P - число фамилий в выборке, N - не фамилий\n",
    "    3) Отсортием выборку по значениям f(x_i)\n",
    "    4) for i in range(1, l):\n",
    "    5)     if y_i == 1:\n",
    "    6)         (FPR_i, TPR_i) = (FPR_{i-1}, TPR_{i-1} + 1/P)\n",
    "    7)     else:\n",
    "    8)         (FPR_i, TPR_i) = (FPR_{i-1} + 1/N, TPR_{i-1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
